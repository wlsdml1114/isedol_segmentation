{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mengui\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jini1114/git/isedol_segmentation/wandb/run-20220517_141626-2v0cf7hx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/engui/segmentation/runs/2v0cf7hx\" target=\"_blank\">clear-music-17</a></strong> to <a href=\"https://wandb.ai/engui/segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/envs/hair_task/lib/python3.6/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1748, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(4.7871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_objectness': tensor(0.0019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0041, device='cuda:0', grad_fn=<DivBackward0>), '_timestamp': 1652764593, '_runtime': 7}\n",
      "\n",
      "\n",
      "\n",
      "tensor(1.6528e+09, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "tensor(1.6528e+09, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch import nn, Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import transforms as T\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import utils\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, image: Tensor,\n",
    "                target: Optional[Dict[str, Tensor]] = None) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class CustomDataset(object):\n",
    "    def __init__(self, root, transforms, num=0):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.name = name\n",
    "        self.files = os.listdir(os.path.join(root,'Masks'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        thrshold = 150\n",
    "        img = cv2.imread(os.path.join(self.root,'Images',self.files[idx][:-3]+'jpg'))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255\n",
    "\n",
    "        mask = cv2.imread(os.path.join(self.root,'Masks',self.files[idx]))\n",
    "        mask = mask[:,:,0]\n",
    "        mask[mask <thrshold] = 0\n",
    "        mask[mask >=thrshold] = 1\n",
    "\n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            if xmin == xmax:\n",
    "                xmax = xmax+1\n",
    "            if ymin == ymax :\n",
    "                ymax = ymax+1\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = torch.tensor(100)\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, wandb, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        target = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        if target[0]['boxes'].size()[0]==0 :\n",
    "            continue\n",
    "        \n",
    "        loss_dict = model(images, target)\n",
    "\n",
    "        wandb.log(loss_dict)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        print(loss_dict)\n",
    "        print('\\n\\n')\n",
    "        print(losses)\n",
    "        print('\\n\\n')\n",
    "        print(losses_reduced)\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        break\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='mask rcnn')    \n",
    "#parser.add_argument('--name', required = True, help='folder name')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "#set root\n",
    "name = 'viichan'\n",
    "data_path = os.path.join('/home/jini1114/git/data/augmentation',name)\n",
    "model_path = '/home/jini1114/git/data/model/'\n",
    "\n",
    "wandb.init(project=\"segmentation\", entity=\"engui\")\n",
    "wandb.run.name = name\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "num_classes = 2\n",
    "dataset = CustomDataset(data_path, get_transform(train=True))\n",
    "\n",
    "batch_size = 2\n",
    "lr = 0.005\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=lr,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"epochs\": num_epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, wandb, print_freq=1)\n",
    "    lr_scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'loss_classifier': 0.8349, 'loss_box_reg': 0.1718, 'loss_mask': 2.8173, 'loss_objectness': 0.0024, 'loss_rpn_box_reg': 0.0054}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': 0.8349,\n",
       " 'loss_box_reg': 0.1718,\n",
       " 'loss_mask': 2.8173,\n",
       " 'loss_objectness': 0.0024,\n",
       " 'loss_rpn_box_reg': 0.0054}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:test[t] for t in test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b15448ea699fca3ef968e46ccc7c03b1e0bb6c8384b306cbbe852e822367af1b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('hair_task')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
